{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as D\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from itertools import cycle\n",
    "from tqdm.auto import trange\n",
    "from copy import deepcopy\n",
    "\n",
    "sys.path.append(\"../lib\")\n",
    "from sw import *\n",
    "from nf.realnvp import *\n",
    "from nf.utils_nf import *\n",
    "\n",
    "from evaluate import posterior_sample_evaluation\n",
    "from datasets import *\n",
    "from data_posterior import LogRegDPTarget, posterior_sample_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_flow(n_epochs, rho_prev, J, create_NF, d, h, \n",
    "                  num_projections, nh, nl, lr, n_samples, device, \n",
    "                  k, plot_loss, sw_approx, max_sliced, reset_NF, use_scheduler):\n",
    "    \"\"\"\n",
    "        Perform gradient descent at time step t\n",
    "        \n",
    "        Inputs:\n",
    "        - n_epochs\n",
    "        - rho_prev: previous NF\n",
    "        - J: functional (taking (x,z,log(det(J(z)))) as inputs)\n",
    "        - h: time step\n",
    "        - num_projections\n",
    "        - nh: number of hidden units\n",
    "        - nl: number of layers\n",
    "        - lr\n",
    "        - device\n",
    "        - k: step\n",
    "        - plot_loss\n",
    "        - sw_approx: use the concentration approximation\n",
    "        - max_sliced: if True, use max SW\n",
    "        - reset_NF: If True, start from a random initialized NF\n",
    "        - use_scheduler: If True, use ReduceLROnPlateau Scheduler\n",
    "        \n",
    "        Outputs:\n",
    "        - rho_{k+1}^h\n",
    "    \"\"\"    \n",
    "\n",
    "    if k>0 and not reset_NF: ## check if it is a NF\n",
    "#        rho_k = deepcopy(rho_prev)\n",
    "        rho_k = create_NF(nh, nl, d=d).to(device)\n",
    "        rho_k.load_state_dict(deepcopy(rho_prev.state_dict()))\n",
    "    else:\n",
    "        rho_k = create_NF(nh, nl, d=d).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(rho_k.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "        \n",
    "    \n",
    "    train_loss = []\n",
    "    sw_loss = []\n",
    "    J_loss = []\n",
    "\n",
    "    for j in range(n_epochs):\n",
    "        z_k = torch.randn(n_samples, d, device=device)\n",
    "        x_k, log_det_k = rho_k(z_k)\n",
    "        x_k = x_k[-1]\n",
    "\n",
    "        if k>0:\n",
    "            z0 = torch.randn(n_samples, d, device=device)\n",
    "            x_prev, log_det_prev = rho_prev(z0)\n",
    "            x_prev = x_prev[-1]\n",
    "        else:\n",
    "            x_prev = rho_prev.sample((n_samples,))\n",
    "\n",
    "        if sw_approx:\n",
    "            sw = sw2_approx(x_k, x_prev, device, u_weights=None, v_weights=None)\n",
    "        elif max_sliced:\n",
    "            sw = max_SW(x_k, x_prev, device, p=2, u_weights=None, v_weights=None)\n",
    "        else:\n",
    "            sw = sliced_wasserstein(x_k, x_prev, num_projections, device, \n",
    "                                    u_weights=None, v_weights=None, p=2)\n",
    "            \n",
    "        if num_projections == 0:\n",
    "            sw *= 0\n",
    "            h = 1/2\n",
    "            \n",
    "        f = J(x_k, z_k, log_det_k)\n",
    "        loss = sw+2*h*f\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for flow in rho_k.flows:\n",
    "            if flow.__class__.__name__ == \"ConvexPotentialFlow\":\n",
    "                flow.icnn.convexify() # clamp weights to be >=0\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        sw_loss.append(sw.item())\n",
    "        J_loss.append(2*h*f.item())\n",
    "        \n",
    "        if use_scheduler:\n",
    "            scheduler.step(f)\n",
    "\n",
    "    if plot_loss:\n",
    "        fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "        ax[0].plot(range(len(train_loss)),train_loss, label=\"Loss\")\n",
    "        L = range(10, len(train_loss))\n",
    "        moving_average = []\n",
    "        for i in range(len(L)):\n",
    "            moving_average.append(np.mean(train_loss[i:i+10]))\n",
    "        ax[0].plot(L, moving_average, label=\"Moving Average\")\n",
    "        ax[0].set_title(\"Full loss\")\n",
    "        ax[0].legend()\n",
    "        \n",
    "        ax[1].plot(sw_loss)\n",
    "        moving_average = []\n",
    "        for i in range(len(L)):\n",
    "            moving_average.append(np.mean(sw_loss[i:i+10]))\n",
    "        ax[1].plot(L, moving_average)\n",
    "        ax[1].set_title(\"SW\")\n",
    "        \n",
    "        ax[2].plot(J_loss)\n",
    "        moving_average = []\n",
    "        for i in range(len(L)):\n",
    "            moving_average.append(np.mean(J_loss[i:i+10]))\n",
    "        ax[2].plot(L, moving_average)\n",
    "        ax[2].set_title(\"2hJ\")\n",
    "        \n",
    "        plt.suptitle(\"k=\"+str(k))\n",
    "        plt.show()\n",
    "        \n",
    "    return rho_k\n",
    "\n",
    "\n",
    "def SWGF_BLR(rho_0, tau, n_step, n_epochs, create_NF, target, d=2, nh=64, nl=5, lrs=1e-5, \n",
    "         num_projections=100, n_samples=500, sw_approx=False, max_sliced=False, reset_NF=False, \n",
    "         device=device, use_scheduler=False, plot_loss=False, tqdm_bar=False):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "        - rho_0\n",
    "        - tau: step size\n",
    "        - n_step: number of t steps\n",
    "        - n_epochs: number of epochs for the optimization (can be a list of size\n",
    "        n_step or an int)\n",
    "        - create_NF: function which return a BaseNormalizingFlow class taking\n",
    "        (nh, nl, d) as inputs\n",
    "        - target: Target posterior for Bayesian Logistic Regression\n",
    "        - nh: number of hidden units\n",
    "        - nl: number of layers\n",
    "        - lrs: learning rate for optimization (can be a list of size n_step or an int)\n",
    "        - num_projections\n",
    "        - n_samples: batch size\n",
    "        - sw_approx: If true, use the SW_2^2 approximation of SW (without projections)\n",
    "        - max_sliced: If True, use max-SW \n",
    "        - reset_nn: If True, start from an unitialized flow\n",
    "        - device\n",
    "        - use_scheduler: If True, use a ReduceLROnPlateau Scheduler\n",
    "        - plot_loss (default False)\n",
    "        - tqdm_bar (default False)\n",
    "    \"\"\"\n",
    "    \n",
    "    Lrho = [rho_0] ## For rho_0, distribution class\n",
    "    \n",
    "    if tqdm_bar:    \n",
    "        pbar = trange(n_step)\n",
    "    else:\n",
    "        pbar = range(n_step)\n",
    "    \n",
    "    for k in pbar:\n",
    "        def V(X):\n",
    "            S = target.sample_data()\n",
    "\n",
    "            init_targ_loss = target.est_log_init_prob(X, reduction='sum')\n",
    "\n",
    "            data_targ_loss = target.len_dataset * target.est_log_data_prob(X, S, reduction='sum')\n",
    "            loss = - init_targ_loss - data_targ_loss\n",
    "            return loss\n",
    "\n",
    "\n",
    "        def J(x, z, log_det):\n",
    "            h = torch.mean(log_likelihood(z, log_det, device), axis=0) ## entropy\n",
    "            v = torch.mean(V(x), axis=0)\n",
    "            return v+h\n",
    "    \n",
    "        if isinstance(n_epochs, np.ndarray):\n",
    "            n_epoch = n_epochs[k].astype(int)\n",
    "        else:\n",
    "            n_epoch = n_epochs\n",
    "\n",
    "        if isinstance(lrs, np.ndarray):\n",
    "            lr = lrs[k]\n",
    "        else:\n",
    "            lr = lrs\n",
    "\n",
    "        rho_k = one_step_flow(n_epoch, Lrho[-1], J, create_NF, d, tau, \n",
    "                                num_projections, nh, nl, lr, n_samples, \n",
    "                                device, k, plot_loss, sw_approx, max_sliced, \n",
    "                                reset_NF, use_scheduler)\n",
    "        \n",
    "        Lrho.append(rho_k)\n",
    "\n",
    "    return Lrho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_blr(dataset_name, dataset_batch_size, lr, ntraining=5,\n",
    "                 h=0.1, t_end=0.5, t_init=0, nh=512, nl=2, epochs=500,\n",
    "                n_projs=1000, tqdm_bar=True, plot_loss=False):\n",
    "    dataset, train_ds, test_ds = get_train_test_datasets(dataset_name)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=dataset_batch_size, shuffle=True)\n",
    "    target = LogRegDPTarget(train_dl, dataset.n_features, device=device, clip_alpha=8)\n",
    "\n",
    "    X_test, y_test = dataset2numpy(test_ds)\n",
    "    \n",
    "    accuracies = []\n",
    "    ts = []\n",
    "    \n",
    "    if tqdm_bar:\n",
    "        pbar = trange(ntraining)\n",
    "    else:\n",
    "        pbar = range(ntraining)\n",
    "        \n",
    "    for k in pbar:\n",
    "        start = time.time()\n",
    "\n",
    "        d = dataset.n_features\n",
    "\n",
    "        n_steps = int(np.ceil((t_end-t_init)/h))\n",
    "\n",
    "        mu0 = torch.zeros(d+1, device=device, dtype=torch.float)\n",
    "        sigma0 = torch.eye(d+1, device=device, dtype=torch.float)\n",
    "        rho_0 = D.MultivariateNormal(mu0, sigma0)\n",
    "\n",
    "        lrs = lr * np.ones(n_steps)\n",
    "\n",
    "        Lrho = SWGF_BLR(rho_0, h, n_step=n_steps, n_epochs=epochs, d=d+1,\n",
    "                    create_NF=create_RealNVP, nh=nh, nl=nl, lrs=lrs, \n",
    "                    num_projections=n_projs, n_samples=1024, plot_loss=plot_loss,\n",
    "                    tqdm_bar=False, use_scheduler=False, target=target)\n",
    "\n",
    "        ts.append(time.time()-start)\n",
    "        \n",
    "        rho = Lrho[-1]\n",
    "        z = torch.randn((4096,d+1), device=device)\n",
    "        ws, _ = rho(z)\n",
    "        w = ws[-1]\n",
    "        acc, _ = posterior_sample_evaluation(w.detach().cpu().numpy(), X_test, y_test)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return accuracies, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9a3315d0fc4b6981765666103d9d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"covtype\", 512, nh=512, nl=2, lr=2e-5, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.7557894374499797, 0.7549977195081022, 0.7561250570122975, 0.7563401977573728, 0.7522267067115307]\n",
      "Mean 0.7550958236878567 Std 0.0015053467883486124\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [108.42422366142273, 112.59758138656616, 97.97873544692993, 99.54449796676636, 99.1012225151062]\n",
      "Mean 103.52925219535828 Std 5.87348770765303\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e64a0b4fb1479fad8a3729f310eaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"german\", 800, lr=1e-4, h=1e-6, t_end=5e-6, plot_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.675, 0.68, 0.69, 0.68, 0.675]\n",
      "Mean 0.68 Std 0.005477225575051626\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [82.31915354728699, 82.52840495109558, 82.5493311882019, 82.40837907791138, 82.97730135917664]\n",
      "Mean 82.5565140247345 Std 0.22635708635375681\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c697da2d54524ba8a25ec1849344bdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"diabetis\", 614, lr=5e-4, h=5e-6, t_end=5e-5, plot_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.7727272727272727, 0.7792207792207793, 0.7792207792207793, 0.7792207792207793, 0.7792207792207793]\n",
      "Mean 0.7779220779220779 Std 0.0025974025974026204\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [124.2893009185791, 113.6352903842926, 124.00571465492249, 124.34977626800537, 124.2599229812622]\n",
      "Mean 122.10800104141235 Std 4.237983864860112\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twonorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09c5efaf013481595de7840380adaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"twonorm\", 1024, lr=1e-4,  h=1e-8, t_end=20e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.9817567567567568, 0.981081081081081, 0.9804054054054054, 0.981081081081081, 0.9797297297297297]\n",
      "Mean 0.9808108108108108 Std 0.0006890566910260536\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [291.88450264930725, 291.63723278045654, 322.92185854911804, 298.90231490135193, 301.83920645713806]\n",
      "Mean 301.43702306747434 Std 11.44963919648696\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ringnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d941c3f597d41809091c4c012df24dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"ringnorm\", 1024, lr=5e-5, h=1e-6, t_end=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.7412162162162163, 0.7412162162162163, 0.7405405405405405, 0.7418918918918919, 0.7398648648648649]\n",
      "Mean 0.7409459459459459 Std 0.0006890566910260363\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [82.74868774414062, 82.9918487071991, 82.74334359169006, 82.88334560394287, 82.34805178642273]\n",
      "Mean 82.74305548667908 Std 0.2180087668528168\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da95aae988e64ebd93411f53544c6fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"banana\", 1024, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.5490566037735849, 0.5433962264150943, 0.569811320754717, 0.5716981132075472, 0.560377358490566]\n",
      "Mean 0.5588679245283019 Std 0.011156034338939231\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [66.71263360977173, 66.6420567035675, 66.59221506118774, 66.75500988960266, 66.54901146888733]\n",
      "Mean 66.6501853466034 Std 0.07556553810915242\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c39d9c747a046b6a465a150157a645c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"splice\", 512, lr=5e-4, nl=5, nh=128,  h=1e-6, t_end=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.8447412353923205, 0.8514190317195326, 0.8497495826377296, 0.8530884808013356, 0.8497495826377296]\n",
      "Mean 0.8497495826377296 Std 0.002793522626157209\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [102.52043581008911, 119.41600966453552, 110.12472009658813, 121.95077538490295, 115.92983675003052]\n",
      "Mean 113.98835554122925 Std 6.972370760971641\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0582ae7ca76d48f8a537030fdc4d7e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"waveform\", 512, nh=128, nl=5, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.777, 0.776, 0.777, 0.775, 0.775]\n",
      "Mean 0.776 Std 0.0008944271909999167\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [120.79881429672241, 121.34904217720032, 120.70495057106018, 120.94201898574829, 119.3961431980133]\n",
      "Mean 120.6381938457489 Std 0.6588718034957246\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ef4006ae404e668bc35eddd22706ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, ts = evaluate_blr(\"image\", 1024, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [0.8205741626794258, 0.8205741626794258, 0.8277511961722488, 0.8157894736842105, 0.8205741626794258]\n",
      "Mean 0.8210526315789475 Std 0.0038277511961722493\n"
     ]
    }
   ],
   "source": [
    "print(\"Results\", acc)\n",
    "print(\"Mean\", np.mean(acc), \"Std\", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [74.73810482025146, 81.63690543174744, 68.53701853752136, 68.74294590950012, 68.55454754829407]\n",
      "Mean 72.4419044494629 Std 5.174215904844543\n"
     ]
    }
   ],
   "source": [
    "print(\"Time\", ts)\n",
    "print(\"Mean\", np.mean(ts), \"Std\", np.std(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
